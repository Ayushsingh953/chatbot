{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfsMBj0MLHEx",
        "outputId": "a6b9d91d-83a2-4274-8f90-6a64d6ead041"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "!pip install langchain_community\n",
        "!pip install langchain\n",
        "!pip install unstrutured\n",
        "!pip install openai\n",
        "!pip insatll chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaY2c-s0j0Z_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzDzspDsN2KM"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjQs2Al8j48v"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "CHROMA_PATH = \"/content/chroma_db\"\n",
        "DATA_PATH = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "collapsed": true,
        "id": "920bIx2pj-ta",
        "outputId": "858026bf-25af-4c7d-9edc-783892c19cfb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "  generate_data_store()\n",
        "\n",
        "\n",
        "def generate_data_store():\n",
        "    documents = load_documents()\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "\n",
        "def load_documents():\n",
        "    print(\"load doc\")\n",
        "    loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\",show_progress=True)\n",
        "    documents = loader.load()\n",
        "    print(\"load doc end\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "def split_text(documents: list[Document]):\n",
        "    print(\"Start\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=500,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "    document = chunks[0]\n",
        "    print(document.page_content)\n",
        "    print(document.metadata)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
        "    )\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
        "\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on0bUUm8kB-7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j1EZHz06kEOx",
        "outputId": "dc75bcf0-153a-48ae-fada-ff34c3562d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Yes, there is an announcement regarding the Reappear/Improvement Examination 2324B (Theory) to be held in the months of March-April 2024. Students who have registered for this examination, both with CA improvement option and without CA improvement option, are advised to check their seating plan in their respective UMS Login. The guidelines/instructions for the examination can be accessed through the following paths: Path for Reappear (Theory) Examination: Important Links ---> Policies, Rules, Instructions, Guidelines & Formats--->Examination Instruction and Guidelines -----> Reappear Examination Guidelines. Path for CA (Theory) Examination: Important Links ---> Policies, Rules, Instructions, Guidelines & Formats--->Examination Instruction and Guidelines -----> CA Improvement Test Guidelines. Students are required to strictly adhere to these guidelines.\n",
            "Sources: ['/content/data/Announcements.txt', '/content/data/Announcements.txt', '/content/data/Announcements.txt']\n"
          ]
        }
      ],
      "source": [
        "CHROMA_PATH = \"/content/chroma_db\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a chtabot made for the Lovely professional university students and your name is LPU Query Bot.\n",
        "Answer the question based on the following context if the user asks about the questions related to the context :\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "Answer the question based on the context and the instructions that has been given to you and\n",
        " while giving the answer be more formal and provide the complete answer\n",
        ": {question}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    query_text = \"Is there any announcement regarding reappear?\"\n",
        "\n",
        "    embedding_function = OpenAIEmbeddings()\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "    # if len(results) == 0 or results[0][1] < 0.7:\n",
        "    #     print(f\"Unable to find matching results.\")\n",
        "    #     return\n",
        "\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "    # print(prompt)\n",
        "\n",
        "    model = ChatOpenAI()\n",
        "    response_text = model.predict(prompt)\n",
        "\n",
        "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "    print(formatted_response)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
